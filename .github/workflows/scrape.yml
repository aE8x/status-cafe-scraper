name: Scrape Status.Cafe Daily

on:
  schedule:
    # Runs at 05:00 UTC every day. You can change this.
    # Use a tool like https://crontab.guru to build your schedule.
    - cron: '0 5 * * *'
  # This allows you to run the job manually from the Actions tab
  workflow_dispatch:

jobs:
  scrape-and-commit:
    runs-on: ubuntu-latest

    steps:
    # Step 1: Checks out your repository's code so the runner can use it
    - name: Check out repo
      uses: actions/checkout@v4
      with:
        # We need to fetch the whole history to load the existing data file
        fetch-depth: 0

    # Step 2: Sets up the Python version we want to use
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    # Step 3: Installs the packages from requirements.txt
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    # Step 4: Runs your actual Python script
    - name: Run the scraper
      run: python status_cafe_scraper.py

    # Step 5: Commits the new/updated data file back to your repository
    - name: Commit and push if there are changes
      run: |
        git config --global user.name "GitHub Actions Bot"
        git config --global user.email "actions-bot@github.com"
        git add data/statuses.json  # IMPORTANT: This path must match your script's output
        # This command attempts to commit. If there are no changes, it will
        # fail. `|| exit 0` ensures the workflow continues successfully.
        git commit -m "Chore: Update scraped data" || exit 0
        git push
